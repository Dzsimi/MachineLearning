{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb2d715",
   "metadata": {},
   "source": [
    "# Setting up the scraping environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d56a2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef94d3",
   "metadata": {},
   "source": [
    "# Looping through the pages to gather article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766012b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# Giving label names that can be used in the URL link\n",
    "labels = ['belfold', 'kulfold', 'gazdasag', 'video', 'eletmod', 'sport', 'techtud', 'after', 'english']\n",
    "pages = 5\n",
    "labels = ['belfold']\n",
    "observations = (pages - 1) * len(labels) * 10\n",
    "list_pattern = [None] * observations\n",
    "\n",
    "links = list_pattern\n",
    "counter = 0\n",
    "for j in tqdm(labels):\n",
    "    for i in range(1,pages):\n",
    "        # Giving current URL link\n",
    "        url = 'https://telex.hu/rovat/' + j + '?oldal=' + str(i)\n",
    "        \n",
    "        # Constructing the soup based on the current URL\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Accessing the article links with CSS selector\n",
    "        elements = soup.select('a.list__item__title')\n",
    "        \n",
    "        # Saving link parts into a list\n",
    "        pattern = r'href=\"([^\"]+)\"'\n",
    "        for k in range(10):\n",
    "            text = str(elements[k])\n",
    "            link = re.findall(pattern, text)\n",
    "            link = 'https://telex.hu' + link[0]\n",
    "            links[counter] = link\n",
    "            counter += 1\n",
    "            \n",
    "#links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731c1af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)\n",
    "links[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa9d23",
   "metadata": {},
   "source": [
    "# Accessing links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bcf0c",
   "metadata": {},
   "source": [
    "### Collencting authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "714689f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2990/2990 [11:48<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "author_primary = [None] * observations\n",
    "author_secondary = [None] * observations\n",
    "author_tertary = [None] * observations\n",
    "author_quaternary = [None] * observations\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(links):\n",
    "    url = i\n",
    "    \n",
    "    # Constructing the soup based on the current URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Accessing the article links with CSS selector\n",
    "    elements = soup.select('a.author__name')\n",
    "    \n",
    "    # Extracting authors into a list\n",
    "    authors_temp = []\n",
    "    for j in range(len(elements)):\n",
    "        \n",
    "        text_temp = str(elements[j])\n",
    "        first_newline_index = text_temp.find('\\n')\n",
    "        second_newline_index = text_temp.find('\\n', first_newline_index + 1)\n",
    "\n",
    "        author = text_temp[first_newline_index + 1:second_newline_index]\n",
    "        author = author.strip()\n",
    "        \n",
    "        authors_temp.append(author)\n",
    "\n",
    "    # Adding authors to the corresponding lists\n",
    "    if len(authors_temp) == 1:\n",
    "        author_primary[counter] = authors_temp[0]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(authors_temp) == 2:\n",
    "        author_primary[counter] = authors_temp[0]\n",
    "        author_secondary[counter] = authors_temp[1]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(authors_temp) == 3:\n",
    "        author_primary[counter] = authors_temp[0]\n",
    "        author_secondary[counter] = authors_temp[1]\n",
    "        author_tertary[counter] = authors_temp[2]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(authors_temp) >= 4:\n",
    "        author_primary[counter] = authors_temp[0]\n",
    "        author_secondary[counter] = authors_temp[1]\n",
    "        author_tertary[counter] = authors_temp[2]\n",
    "        author_quaternary[counter] = authors_temp[3]\n",
    "        counter += 1\n",
    "    \n",
    "#author_secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3ce75173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2990"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_primary)\n",
    "#author_primary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35609f42",
   "metadata": {},
   "source": [
    "### Collecting title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aa6236e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2990/2990 [04:57<00:00, 10.04it/s]\n"
     ]
    }
   ],
   "source": [
    "titles = [None] * observations\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(links):\n",
    "    url = i\n",
    "    \n",
    "    # Constructing the soup based on the current URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Accessing the article links with CSS selector\n",
    "    elements = soup.select('h1')\n",
    "    \n",
    "    # Extracting title\n",
    "    title = str(elements[0])[5:-5]\n",
    "    title = title.strip()\n",
    "    titles[counter] = title\n",
    "    counter += 1\n",
    "    \n",
    "#titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198af23",
   "metadata": {},
   "source": [
    "### Collecting tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be453309",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2990/2990 [05:01<00:00,  9.93it/s]\n"
     ]
    }
   ],
   "source": [
    "tag_primary = [None] * observations\n",
    "tag_secondary = [None] * observations\n",
    "tag_tertary = [None] * observations\n",
    "tag_quaternary = [None] * observations\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(links):\n",
    "    url = i\n",
    "    \n",
    "    # Constructing the soup based on the current URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Accessing the article links with CSS selector\n",
    "    elements = soup.select('div.title-section__tags.content-wrapper__child')\n",
    "    \n",
    "    # Extracting tags into a list\n",
    "    tags_temp = []\n",
    "    for j in range(len(elements)):\n",
    "        \n",
    "        text_temp = str(elements[j])\n",
    "        first_newline_index = text_temp.find('\\n')\n",
    "        second_newline_index = text_temp.find('\\n', first_newline_index + 1)\n",
    "\n",
    "        tag = text_temp[first_newline_index + 1:second_newline_index]\n",
    "        tag = tag.strip()\n",
    "        \n",
    "        tags_temp.append(tag)\n",
    "        \n",
    "    # Adding tags to the corresponding lists\n",
    "    if len(tags_temp) == 1:\n",
    "        tag_primary[counter] = tags_temp[0]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(tags_temp) == 2:\n",
    "        tag_primary[counter] = tags_temp[0]\n",
    "        tag_secondary[counter] = tags_temp[1]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(tags_temp) == 3:\n",
    "        tag_primary[counter] = tags_temp[0]\n",
    "        tag_secondary[counter] = tags_temp[1]\n",
    "        tag_tertary[counter] = tags_temp[2]\n",
    "        counter += 1\n",
    "        \n",
    "    if len(tags_temp) >= 4:\n",
    "        tag_primary[counter] = tags_temp[0]\n",
    "        tag_secondary[counter] = tags_temp[1]\n",
    "        tag_tertary[counter] = tags_temp[2]\n",
    "        tag_quaternary[counter] = tags_temp[3]\n",
    "        counter += 1\n",
    "    \n",
    "#tag_secondary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cedb49",
   "metadata": {},
   "source": [
    "### Collecting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [None] * observations\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(links):\n",
    "    url = i\n",
    "    \n",
    "    # Constructing the soup based on the current URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Accessing the article links with CSS selector\n",
    "    elements = soup.select('p')\n",
    "    \n",
    "    # Extracting text into a string\n",
    "    text = ''\n",
    "    if len(str(elements[4])[3:-4].strip()) < 10:\n",
    "        elements = elements[1:]\n",
    "    for j in range(4,len(elements)):\n",
    "        if str(elements[j])[3] != '<':\n",
    "            text = text + ' ' + str(elements[j])[3:-5]\n",
    "            \n",
    "    # Deleting longer links from the text\n",
    "    pattern = r'<a\\s+href=\"[^\"]*\"\\s+target=\"_blank\">'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    \n",
    "    pattern = r'<a\\s+href=\"[^\"]*\"[^>]*>'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Deleting additional shit from the text\n",
    "    substring_to_delete = '</a>'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    substring_to_delete = '<i>'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    substring_to_delete = '</i>'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    substring_to_delete = '<b>'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    substring_to_delete = '</b>'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    substring_to_delete = 'class=\"article-html-content article__lead\">\\n'\n",
    "    text = text.replace(substring_to_delete, '')\n",
    "    \n",
    "    text = text.strip()\n",
    "    texts[counter] = text\n",
    "    counter += 1\n",
    "    \n",
    "#texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8f914",
   "metadata": {},
   "source": [
    "# Creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18910081",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.DataFrame({'link': links,\n",
    "                         'titles': titles,\n",
    "                         'author_primary': author_primary,\n",
    "                         'author_secondary': author_secondary,\n",
    "                         'author_tertary': author_tertary,\n",
    "                         'author_quaternary': author_quaternary,\n",
    "                         'tag_primary': tag_primary,\n",
    "                         'tag_secondary': tag_secondary,\n",
    "                         'tag_tertary': tag_tertary,\n",
    "                         'tag_quaternary': tag_quaternary,\n",
    "                         'texts': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv('articles.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31aace",
   "metadata": {},
   "source": [
    "# Creating the whole scraping code in a single loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf00f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Giving label names that can be used in the URL link\n",
    "labels = ['belfold', 'kulfold', 'gazdasag', 'video', 'eletmod', 'sport', 'techtud', 'after', 'english']\n",
    "labels = ['belfold']\n",
    "#observations = (pages - 1) * len(labels) * 10\n",
    "#list_pattern = [None] * observations\n",
    "year_substring = '/2022/'\n",
    "\n",
    "links = ['Imi']\n",
    "\n",
    "for l in labels:\n",
    "    page = 0\n",
    "    while year_substring not in links[-1]:\n",
    "        # Downloading links\n",
    "        titles = []\n",
    "        if len(links) == 1:\n",
    "            links = []\n",
    "        for i in range(1,51):\n",
    "            page += 1\n",
    "            # Giving current URL link\n",
    "            url = 'https://telex.hu/rovat/' + l + '?oldal=' + str(page)\n",
    "            \n",
    "            # Constructing the soup based on the current URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Accessing the article links with CSS selector\n",
    "            elements = soup.select('a.list__item__title')\n",
    "            \n",
    "            # Saving link parts into a list\n",
    "            pattern = r'href=\"([^\"]+)\"'\n",
    "            for k in range(10):\n",
    "                text = str(elements[k])\n",
    "                link = re.findall(pattern, text)\n",
    "                link = 'https://telex.hu' + link[0]\n",
    "                links.append(link)\n",
    "        # Accessing links\n",
    "        # Collecting authors\n",
    "        author_primary = []\n",
    "        author_secondary = []\n",
    "        author_tertary = []\n",
    "        author_quaternary = []\n",
    "        \n",
    "        for i in links:\n",
    "            url = i\n",
    "\n",
    "            # Constructing the soup based on the current URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Accessing the article links with CSS selector\n",
    "            elements = soup.select('a.author__name')\n",
    "\n",
    "            # Extracting authors into a list\n",
    "            authors_temp = []\n",
    "            \n",
    "            for j in range(len(elements)):\n",
    "\n",
    "                text_temp = str(elements[j])\n",
    "                first_newline_index = text_temp.find('\\n')\n",
    "                second_newline_index = text_temp.find('\\n', first_newline_index + 1)\n",
    "\n",
    "                author = text_temp[first_newline_index + 1:second_newline_index]\n",
    "                author = author.strip()\n",
    "\n",
    "                authors_temp.append(author)\n",
    "\n",
    "            # Adding authors to the corresponding lists\n",
    "            if len(authors_temp) == 1:\n",
    "                author_primary.append(authors_temp[0])\n",
    "                author_secondary.append(None)\n",
    "                author_tertary.append(None)\n",
    "                author_quaternary.append(None)\n",
    "\n",
    "            if len(authors_temp) == 2:\n",
    "                author_primary.append(authors_temp[0])\n",
    "                author_secondary.append(authors_temp[1])\n",
    "                author_tertary.append(None)\n",
    "                author_quaternary.append(None)\n",
    "\n",
    "            if len(authors_temp) == 3:\n",
    "                author_primary.append(authors_temp[0])\n",
    "                author_secondary.append(authors_temp[1])\n",
    "                author_tertary.append(authors_temp[2])\n",
    "                author_quaternary.append(None)\n",
    "\n",
    "            if len(authors_temp) >= 4:\n",
    "                author_primary.append(authors_temp[0])\n",
    "                author_secondary.append(authors_temp[1])\n",
    "                author_tertary.append(authors_temp[2])\n",
    "                author_quaternary.append(authors_temp[3])\n",
    "            \n",
    "        # Collecting titles\n",
    "        titles = []\n",
    "            \n",
    "        for i in links:\n",
    "            url = i\n",
    "\n",
    "            # Constructing the soup based on the current URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Accessing the article links with CSS selector\n",
    "            elements = soup.select('h1')\n",
    "\n",
    "            # Extracting title\n",
    "            title = str(elements[0])[5:-5]\n",
    "            title = title.strip()\n",
    "            titles.append(title)\n",
    "    \n",
    "        # Collecting tags\n",
    "        tag_primary = []\n",
    "        tag_secondary = []\n",
    "        tag_tertary = []\n",
    "        tag_quaternary = []\n",
    "\n",
    "        for i in links:\n",
    "            url = i\n",
    "\n",
    "            # Constructing the soup based on the current URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Accessing the article links with CSS selector\n",
    "            elements = soup.select('div.title-section__tags.content-wrapper__child')\n",
    "\n",
    "            # Extracting tags into a list\n",
    "            tags_temp = []\n",
    "            for j in range(len(elements)):\n",
    "\n",
    "                text_temp = str(elements[j])\n",
    "                first_newline_index = text_temp.find('\\n')\n",
    "                second_newline_index = text_temp.find('\\n', first_newline_index + 1)\n",
    "\n",
    "                tag = text_temp[first_newline_index + 1:second_newline_index]\n",
    "                tag = tag.strip()\n",
    "\n",
    "                tags_temp.append(tag)\n",
    "\n",
    "            # Adding tags to the corresponding lists\n",
    "            if len(tags_temp) == 1:\n",
    "                tag_primary.append(tags_temp[0])\n",
    "                tag_secondary.append(None)\n",
    "                tag_tertary.append(None)\n",
    "                tag_quaternary.append(None)\n",
    "                \n",
    "            if len(tags_temp) == 2:\n",
    "                tag_primary.append(tags_temp[0])\n",
    "                tag_secondary.append(tags_temp[1])\n",
    "                tag_tertary.append(None)\n",
    "                tag_quaternary.append(None)\n",
    "\n",
    "            if len(tags_temp) == 3:\n",
    "                tag_primary.append(tags_temp[0])\n",
    "                tag_secondary.append(tags_temp[1])\n",
    "                tag_tertary.append(tags_temp[2])\n",
    "                tag_quaternary.append(None)\n",
    "\n",
    "            if len(tags_temp) >= 4:\n",
    "                tag_primary.append(tags_temp[0])\n",
    "                tag_secondary.append(tags_temp[1])\n",
    "                tag_tertary.append(tags_temp[2])\n",
    "                tag_quaternary.append(tags_temp[3])\n",
    "                \n",
    "        # Collecting texts\n",
    "        texts = []\n",
    "        \n",
    "        for i in links:\n",
    "            url = i\n",
    "\n",
    "            # Constructing the soup based on the current URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Accessing the article links with CSS selector\n",
    "            elements = soup.select('p')\n",
    "\n",
    "            # Extracting text into a string\n",
    "            text = ''\n",
    "            if len(str(elements[4])[3:-4].strip()) < 10:\n",
    "                elements = elements[1:]\n",
    "            for j in range(4,len(elements)):\n",
    "                if str(elements[j])[3] != '<':\n",
    "                    text = text + ' ' + str(elements[j])[3:-5]\n",
    "\n",
    "            # Deleting longer links from the text\n",
    "            pattern = r'<a\\s+href=\"[^\"]*\"\\s+target=\"_blank\">'\n",
    "            text = re.sub(pattern, '', text)\n",
    "\n",
    "            pattern = r'<a\\s+href=\"[^\"]*\"[^>]*>'\n",
    "            text = re.sub(pattern, '', text)\n",
    "\n",
    "            # Deleting additional shit from the text\n",
    "            substring_to_delete = '</a>'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "            substring_to_delete = '<i>'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "            substring_to_delete = '</i>'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "            substring_to_delete = '<b>'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "            substring_to_delete = '</b>'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "            substring_to_delete = 'class=\"article-html-content article__lead\">\\n'\n",
    "            text = text.replace(substring_to_delete, '')\n",
    "\n",
    "            text = text.strip()\n",
    "            texts.append(text)\n",
    "            \n",
    "        # Writing dataframe\n",
    "        articles = pd.DataFrame({'link': links,\n",
    "                         'titles': titles,\n",
    "                         'author_primary': author_primary,\n",
    "                         'author_secondary': author_secondary,\n",
    "                         'author_tertary': author_tertary,\n",
    "                         'author_quaternary': author_quaternary,\n",
    "                         'tag_primary': tag_primary,\n",
    "                         'tag_secondary': tag_secondary,\n",
    "                         'tag_tertary': tag_tertary,\n",
    "                         'tag_quaternary': tag_quaternary,\n",
    "                         'texts': texts})\n",
    "        \n",
    "        # Saving the dataframe\n",
    "        end_page = str(page)\n",
    "        start_page = str(page - 49)\n",
    "        \n",
    "        folder_path = 'C:/Users/imre2/Desktop/'\n",
    "        saving_name = os.path.join(folder_path, f\"telex_{l}_{start_page}_{end_page}.csv\")\n",
    "        \n",
    "        articles.to_csv(saving_name, index=False)\n",
    "        \n",
    "        print(l, page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
